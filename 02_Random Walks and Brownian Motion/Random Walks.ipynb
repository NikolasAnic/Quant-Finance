{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brownian Motion\n",
    "\n",
    "---\n",
    "\n",
    "## Scaled Random Walks\n",
    "\n",
    "### Symmetric Random Walk\n",
    "To construct a **Brownian motion**, we begin with a **symmetric random walk**. This process involves repeatedly tossing a **fair coin**, where $p = \\frac{1}{2}$ (probability of heads) and $q = \\frac{1}{2}$ (probability of tails). Let the outcomes of the tosses be represented as $w = w_1 w_2 w_3 \\dots$, with $w_n$ being the result of the $n$-th toss. Define the random variables:\n",
    "$$\n",
    "X_j =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } w_j = H, \\\\\n",
    "-1 & \\text{if } w_j = T,\n",
    "\\end{cases}\n",
    "$$\n",
    "and construct the random walk:\n",
    "$$\n",
    "M_0 = 0, \\quad M_k = \\sum_{j=1}^k X_j, \\quad k = 1, 2, \\dots.\n",
    "$$\n",
    "Here, $M_k$ represents the cumulative position after $k$ tosses. At each step, the random walk moves up or down by one unit with equal probability.\n",
    "\n",
    "---\n",
    "\n",
    "### Increments of the Symmetric Random Walk\n",
    "A **symmetric random walk** has **independent increments**. For any nonnegative integers $0 = k_0 < k_1 < \\dots < k_m$, the increments:\n",
    "$$\n",
    "M_{k_{i+1}} - M_{k_i} = \\sum_{j=k_i+1}^{k_{i+1}} X_j,\n",
    "$$\n",
    "are independent random variables. Each increment represents the change in position of the random walk over the interval $[k_i, k_{i+1}]$.\n",
    "\n",
    "--- \n",
    "\n",
    "#### **Expected Value**: \n",
    "  $$\n",
    "  \\mathbb{E}[M_{k_{i+1}} - M_{k_i}] = 0.\n",
    "  $$\n",
    "\n",
    "##### Explanation:\n",
    "1. **Definition of the Increment**:  \n",
    "   By definition:\n",
    "   $$\n",
    "   M_{k_{i+1}} - M_{k_i} = \\sum_{j=k_i+1}^{k_{i+1}} X_j,\n",
    "   $$\n",
    "   where $X_j$ represents the outcome of the $j$-th step in the random walk.  \n",
    "   Each step $X_j$ takes the value $+1$ (for heads) or $-1$ (for tails).\n",
    "\n",
    "2. **Expected Value of Each Step**:  \n",
    "   Each $X_j$ is an independent random variable with:\n",
    "   $$\n",
    "   \\mathbb{E}[X_j] = 1 \\cdot \\frac{1}{2} + (-1) \\cdot \\frac{1}{2} = 0.\n",
    "   $$\n",
    "\n",
    "3. **Linearity of Expectation**:  \n",
    "   Using the linearity of expectation:\n",
    "   $$\n",
    "   \\mathbb{E}[M_{k_{i+1}} - M_{k_i}] = \\mathbb{E}\\left[\\sum_{j=k_i+1}^{k_{i+1}} X_j\\right] \n",
    "   = \\sum_{j=k_i+1}^{k_{i+1}} \\mathbb{E}[X_j] = 0.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Variance**:\n",
    "  $$\n",
    "  \\text{Var}(M_{k_{i+1}} - M_{k_i}) = k_{i+1} - k_i.\n",
    "  $$\n",
    "\n",
    "##### Explanation:\n",
    "1. **Definition of the Increment**:  \n",
    "   From the definition:\n",
    "   $$\n",
    "   M_{k_{i+1}} - M_{k_i} = \\sum_{j=k_i+1}^{k_{i+1}} X_j.\n",
    "   $$\n",
    "\n",
    "2. **Variance of Each Step**:  \n",
    "   Each $X_j$ is independent, so:\n",
    "   $$\n",
    "   \\text{Var}(X_j) = \\mathbb{E}[X_j^2] - (\\mathbb{E}[X_j])^2.\n",
    "   $$  \n",
    "   Since $X_j^2 = 1$ (as $X_j$ is either $+1$ or $-1$), we have:\n",
    "   $$\n",
    "   \\text{Var}(X_j) = 1 - 0^2 = 1.\n",
    "   $$\n",
    "\n",
    "3. **Additivity of Variance**:  \n",
    "   Because the $X_j$ are independent, the variance of the sum is the sum of the variances:\n",
    "   $$\n",
    "   \\text{Var}(M_{k_{i+1}} - M_{k_i}) = \\text{Var}\\left(\\sum_{j=k_i+1}^{k_{i+1}} X_j\\right) \n",
    "   = \\sum_{j=k_i+1}^{k_{i+1}} \\text{Var}(X_j).\n",
    "   $$\n",
    "\n",
    "4. **Result**:  \n",
    "   Since there are $k_{i+1} - k_i$ terms in the sum, each with variance 1:\n",
    "   $$\n",
    "   \\text{Var}(M_{k_{i+1}} - M_{k_i}) = k_{i+1} - k_i.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Martingale Property of the Symmetric Random Walk**\n",
    "\n",
    "The symmetric random walk satisfies the **martingale property**, which states that the expected value of the future position, given all past information, equals the current position. For $k < \\ell$, we compute:\n",
    "$$\n",
    "\\mathbb{E}[M_\\ell \\mid \\mathcal{F}_k] = \\mathbb{E}[(M_\\ell - M_k) + M_k \\mid \\mathcal{F}_k].\n",
    "$$\n",
    "\n",
    "\n",
    "1. **Breakdown of the Expectation**:  \n",
    "   By the definition of the symmetric random walk:\n",
    "   $$\n",
    "   M_\\ell = (M_\\ell - M_k) + M_k.\n",
    "   $$  \n",
    "   The term $(M_\\ell - M_k)$ represents the increment of the random walk from step $k$ to step $\\ell$.\n",
    "\n",
    "2. **Linearity of Expectation**:  \n",
    "   Using the linearity of conditional expectation, we can separate the terms:\n",
    "   $$\n",
    "   \\mathbb{E}[M_\\ell \\mid \\mathcal{F}_k] = \\mathbb{E}[M_\\ell - M_k \\mid \\mathcal{F}_k] + \\mathbb{E}[M_k \\mid \\mathcal{F}_k].\n",
    "   $$\n",
    "\n",
    "3. **Independence of Increments**:  \n",
    "   The increment $(M_\\ell - M_k)$ depends only on the coin tosses between steps $k$ and $\\ell$. These tosses are independent of the information $\\mathcal{F}_k$ (the past up to step $k$). Therefore:\n",
    "   $$\n",
    "   \\mathbb{E}[M_\\ell - M_k \\mid \\mathcal{F}_k] = \\mathbb{E}[M_\\ell - M_k] = 0,\n",
    "   $$  \n",
    "   since the symmetric random walk has zero expected value for each step.\n",
    "\n",
    "4. **Measurability of $M_k$**:  \n",
    "   The term $M_k$ is $\\mathcal{F}_k$-measurable, meaning that its value is already determined by the information $\\mathcal{F}_k$. Thus:\n",
    "   $$\n",
    "   \\mathbb{E}[M_k \\mid \\mathcal{F}_k] = M_k.\n",
    "   $$\n",
    "\n",
    "5. **Conclusion**:  \n",
    "   Substituting these results back into the equation:\n",
    "   $$\n",
    "   \\mathbb{E}[M_\\ell \\mid \\mathcal{F}_k] = \\mathbb{E}[M_\\ell - M_k \\mid \\mathcal{F}_k] + \\mathbb{E}[M_k \\mid \\mathcal{F}_k] = 0 + M_k = M_k.\n",
    "   $$\n",
    "\n",
    "This proves that the symmetric random walk satisfies the **martingale property**. \n",
    "\n",
    "#### **Intuition**:\n",
    "The martingale property indicates that the symmetric random walk has **no tendency to rise or fall** over time. The expected future position, conditioned on all current information, is simply the current position. This lack of bias confirms that the random walk is a **fair game**.\n",
    "\n",
    "---\n",
    "\n",
    "### Quadratic Variation of the Symmetric Random Walk\n",
    "The **quadratic variation** of the symmetric random walk up to time $k$ is:\n",
    "$$\n",
    "[M, M]_k = \\sum_{j=1}^k (M_j - M_{j-1})^2 = k.\n",
    "$$\n",
    "- Each one-step increment $(M_j - M_{j-1})^2 = 1$, so summing over all steps gives $k$. This is b/c each outcome can either be -1 or +1, but the quadratic result of both is always just 1.\n",
    "- Quadratic variation is computed **path-by-path**, and its value does not depend on probabilities of outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "### Scaled Symmetric Random Walk\n",
    "To approximate Brownian motion, we define the **scaled symmetric random walk**:\n",
    "$$\n",
    "W^{(n)}(t) = \\frac{1}{\\sqrt{n}} M_{nt}, \\quad t \\geq 0.\n",
    "$$\n",
    "If $nt$ is not an integer, $W^{(n)}(t)$ is interpolated linearly between the nearest integers. \n",
    "\n",
    "#### **Independent Increments** of scaled symmetric random walks:\n",
    "\n",
    "The scaled symmetric random walk $W^{(n)}(t)$ is defined by scaling a symmetric random walk. The increments are given as:\n",
    "$$ \n",
    "W^{(n)}(t_2) - W^{(n)}(t_1) = \\frac{1}{\\sqrt{n}} \\sum_{j=nt_1+1}^{nt_2} X_j,\n",
    "$$\n",
    "where $X_j$ are independent random variables that take values $1$ or $-1$ with equal probability.\n",
    "\n",
    "- **Why the increments are independent**:  \n",
    "  The increments depend on disjoint sets of random variables $X_j$. For example, if $t_1 < t_2$, the random variables corresponding to the increment $\\sum_{j=nt_1+1}^{nt_2} X_j$ are independent of those corresponding to earlier time intervals. Since $X_j$ are independent, increments over non-overlapping intervals are independent as well.\n",
    "\n",
    "\n",
    "#### **Expected Value** of scaled symmetric random walks:\n",
    "The expected value of the increment $W^{(n)}(t_2) - W^{(n)}(t_1)$ is:\n",
    "$$\n",
    "\\mathbb{E}[W^{(n)}(t_2) - W^{(n)}(t_1)] = \\mathbb{E}\\left[\\frac{1}{\\sqrt{n}} \\sum_{j=nt_1+1}^{nt_2} X_j\\right].\n",
    "$$\n",
    "\n",
    "- **Linearity of expectation**:  \n",
    "  Using the linearity of expectation:\n",
    "  $$\n",
    "  \\mathbb{E}[W^{(n)}(t_2) - W^{(n)}(t_1)] = \\frac{1}{\\sqrt{n}} \\sum_{j=nt_1+1}^{nt_2} \\mathbb{E}[X_j].\n",
    "  $$\n",
    "\n",
    "- **Expected value of $X_j$**:  \n",
    "  Since each $X_j$ takes values $1$ and $-1$ with equal probability, we have:\n",
    "  $$\n",
    "  \\mathbb{E}[X_j] = 0.\n",
    "  $$\n",
    "\n",
    "- **Conclusion**:  \n",
    "  Therefore:\n",
    "  $$\n",
    "  \\mathbb{E}[W^{(n)}(t_2) - W^{(n)}(t_1)] = 0.\n",
    "  $$\n",
    "\n",
    "#### **Variance** of scaled symmetric random walks:\n",
    "The variance of the increment $W^{(n)}(t_2) - W^{(n)}(t_1)$ is:\n",
    "$$\n",
    "\\text{Var}(W^{(n)}(t_2) - W^{(n)}(t_1)) = \\text{Var}\\left(\\frac{1}{\\sqrt{n}} \\sum_{j=nt_1+1}^{nt_2} X_j\\right).\n",
    "$$\n",
    "\n",
    "- **Variance of a scaled sum**:  \n",
    "  Using the property of variance for scaled sums:\n",
    "  $$\n",
    "  \\text{Var}\\left(\\frac{1}{\\sqrt{n}} \\sum_{j=nt_1+1}^{nt_2} X_j\\right) = \\frac{1}{n} \\sum_{j=nt_1+1}^{nt_2} \\text{Var}(X_j).\n",
    "  $$\n",
    "\n",
    "- **Variance of $X_j$**:  \n",
    "  Each $X_j$ has variance:\n",
    "  $$\n",
    "  \\text{Var}(X_j) = \\mathbb{E}[X_j^2] - (\\mathbb{E}[X_j])^2 = 1 - 0 = 1.\n",
    "  $$\n",
    "\n",
    "- **Number of terms in the sum**:  \n",
    "  The number of terms in the summation $\\sum_{j=nt_1+1}^{nt_2} X_j$ is $n(t_2 - t_1)$.\n",
    "\n",
    "- **Conclusion**:  \n",
    "  Substituting back, we get:\n",
    "  $$\n",
    "  \\text{Var}(W^{(n)}(t_2) - W^{(n)}(t_1)) = \\frac{1}{n} \\cdot n(t_2 - t_1) = t_2 - t_1.\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Martingale Property:\n",
    "The martingale property states:\n",
    "$$\n",
    "\\mathbb{E}[W^{(n)}(t) \\mid \\mathcal{F}_s] = W^{(n)}(s), \\quad 0 \\leq s \\leq t.\n",
    "$$\n",
    "\n",
    "- **What it means**:  \n",
    "  The value of the scaled random walk at time $t$, when conditioned on the information available at time $s$ (denoted by $\\mathcal{F}_s$, the $\\sigma$-algebra generated by the first $ns$ coin tosses), is equal to its value at time $s$.  \n",
    "  This implies that the scaled random walk has **no drift** or **tendency to rise or fall** between $s$ and $t$.\n",
    "\n",
    "- **Proof**:  \n",
    "  The scaled random walk is defined as:\n",
    "  $$\n",
    "  W^{(n)}(t) = \\frac{1}{\\sqrt{n}} \\sum_{j=1}^{nt} X_j,\n",
    "  $$\n",
    "  where $X_j$ are independent random variables taking values $1$ or $-1$ with equal probability.\n",
    "\n",
    "  For $s \\leq t$, we decompose $W^{(n)}(t)$:\n",
    "  $$\n",
    "  W^{(n)}(t) = W^{(n)}(s) + \\frac{1}{\\sqrt{n}} \\sum_{j=ns+1}^{nt} X_j.\n",
    "  $$\n",
    "\n",
    "  The first term, $W^{(n)}(s)$, depends on the coin tosses up to time $s$, so it is $\\mathcal{F}_s$-measurable. The second term, $\\frac{1}{\\sqrt{n}} \\sum_{j=ns+1}^{nt} X_j$, depends on coin tosses after time $s$ and is independent of $\\mathcal{F}_s$.\n",
    "\n",
    "  Taking the conditional expectation:\n",
    "  $$\n",
    "  \\mathbb{E}[W^{(n)}(t) \\mid \\mathcal{F}_s] = W^{(n)}(s) + \\mathbb{E}\\left[\\frac{1}{\\sqrt{n}} \\sum_{j=ns+1}^{nt} X_j \\mid \\mathcal{F}_s\\right].\n",
    "  $$\n",
    "  Since the second term has an expected value of zero (because $\\mathbb{E}[X_j] = 0$):\n",
    "  $$\n",
    "  \\mathbb{E}[W^{(n)}(t) \\mid \\mathcal{F}_s] = W^{(n)}(s).\n",
    "  $$\n",
    "\n",
    "  This confirms that $W^{(n)}(t)$ is a **martingale**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Quadratic Variation**:\n",
    "The quadratic variation of the scaled random walk is:\n",
    "$$\n",
    "[W^{(n)}, W^{(n)}]_t = t.\n",
    "$$\n",
    "\n",
    "- **What it means**:  \n",
    "  The quadratic variation tracks the accumulated squared increments of the scaled random walk over time, computed path-by-path. For $W^{(n)}(t)$, it reflects the total variability along the path.\n",
    "\n",
    "- **Definition**:  \n",
    "  For $W^{(n)}(t)$, the quadratic variation up to time $t$ is:\n",
    "  $$\n",
    "  [W^{(n)}, W^{(n)}]_t = \\sum_{j=1}^{nt} \\left[W^{(n)}\\left(\\frac{j}{n}\\right) - W^{(n)}\\left(\\frac{j-1}{n}\\right)\\right]^2.\n",
    "  $$\n",
    "\n",
    "- **Simplification**:  \n",
    "  Each increment $W^{(n)}\\left(\\frac{j}{n}\\right) - W^{(n)}\\left(\\frac{j-1}{n}\\right)$ is:\n",
    "  $$\n",
    "  \\frac{1}{\\sqrt{n}} X_j,\n",
    "  $$\n",
    "  where $X_j$ takes values $1$ or $-1$ with equal probability.\n",
    "\n",
    "  Squaring this:\n",
    "  $$\n",
    "  \\left[W^{(n)}\\left(\\frac{j}{n}\\right) - W^{(n)}\\left(\\frac{j-1}{n}\\right)\\right]^2 = \\frac{1}{n}.\n",
    "  $$\n",
    "\n",
    "  Summing over $nt$ terms:\n",
    "  $$\n",
    "  [W^{(n)}, W^{(n)}]_t = \\sum_{j=1}^{nt} \\frac{1}{n} = t.\n",
    "  $$\n",
    "\n",
    "- **Interpretation**:  \n",
    "  The quadratic variation measures the accumulated squared changes along a single path of the scaled random walk. The result $[W^{(n)}, W^{(n)}]_t = t$ shows that the quadratic variation grows linearly with time, regardless of the specific path taken.\n",
    "\n",
    "---\n",
    "\n",
    "### Limiting Distribution of the Scaled Random Walk (Central Limit)\n",
    "\n",
    "The distribution of $W^{(n)}(t)$, which is a scaled version of a symmetric random walk, approaches the normal distribution $\\mathcal{N}(0, t)$ as $n$ increases. In other words, the scaled random walk converges to a Brownian motion $W(t)$ with zero mean and variance $t$.\n",
    "\n",
    "The scaled random walk $W^{(n)}(t)$ is constructed as:\n",
    "$$\n",
    "W^{(n)}(t) = \\frac{1}{\\sqrt{n}} \\sum_{j=1}^{nt} X_j,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $X_j$ are independent random variables representing steps of a symmetric random walk, taking values $+1$ or $-1$ with equal probabilities,\n",
    "- $t$ is the scaled time parameter.\n",
    "\n",
    "We aim to show that the distribution of $W^{(n)}(t)$ converges to the normal distribution $\\mathcal{N}(0, t)$ as $n \\to \\infty$.\n",
    "\n",
    "### **Key Properties of $W^{(n)}(t)$**\n",
    "\n",
    "#### 1. **Expected Value**:\n",
    "The expected value of $W^{(n)}(t)$ is:\n",
    "$$\n",
    "\\mathbb{E}[W^{(n)}(t)] = \\frac{1}{\\sqrt{n}} \\mathbb{E}\\left[\\sum_{j=1}^{nt} X_j \\right].\n",
    "$$\n",
    "Since $\\mathbb{E}[X_j] = 0$ for all $j$:\n",
    "$$\n",
    "\\mathbb{E}[W^{(n)}(t)] = \\frac{1}{\\sqrt{n}} \\cdot nt \\cdot 0 = 0.\n",
    "$$\n",
    "\n",
    "Thus, $W^{(n)}(t)$ has **mean 0**.\n",
    "\n",
    "#### 2. **Variance**:\n",
    "The variance of $W^{(n)}(t)$ is:\n",
    "$$\n",
    "\\text{Var}(W^{(n)}(t)) = \\frac{1}{n} \\cdot \\text{Var}\\left(\\sum_{j=1}^{nt} X_j\\right).\n",
    "$$\n",
    "Since the $X_j$ are independent with $\\text{Var}(X_j) = 1$:\n",
    "$$\n",
    "\\text{Var}(W^{(n)}(t)) = \\frac{1}{n} \\cdot \\sum_{j=1}^{nt} \\text{Var}(X_j) = \\frac{1}{n} \\cdot nt = t.\n",
    "$$\n",
    "\n",
    "Thus, $W^{(n)}(t)$ has **variance $t$**.\n",
    "\n",
    "### **Central Limit Theorem**\n",
    "\n",
    "By the Central Limit Theorem, the sum of $nt$ independent and identically distributed random variables $X_j$, scaled appropriately, converges to a normal distribution. Specifically:\n",
    "1. For $S_{nt} = \\sum_{j=1}^{nt} X_j$, we know:\n",
    "   - $\\mathbb{E}[S_{nt}] = 0$,\n",
    "   - $\\text{Var}(S_{nt}) = nt$.\n",
    "2. Scaling by $\\frac{1}{\\sqrt{n}}$, the random variable $W^{(n)}(t) = \\frac{1}{\\sqrt{n}} S_{nt}$ has:\n",
    "   - $\\mathbb{E}[W^{(n)}(t)] = 0$,\n",
    "   - $\\text{Var}(W^{(n)}(t)) = t$.\n",
    "\n",
    "By the Central Limit Theorem, as $n \\to \\infty$, the scaled random walk $W^{(n)}(t)$ converges in distribution to a normal random variable with mean 0 and variance $t$:\n",
    "$$\n",
    "W^{(n)}(t) \\xrightarrow{d} \\mathcal{N}(0, t).\n",
    "$$\n",
    "\n",
    "### **Convergence to Brownian Motion**\n",
    "\n",
    "The convergence of $W^{(n)}(t)$ to $\\mathcal{N}(0, t)$ at fixed times $t$ implies that the process $W^{(n)}(t)$, viewed as a collection of random variables indexed by time $t$, converges to a **Brownian motion** $W(t)$.\n",
    "\n",
    "#### Brownian Motion Properties:\n",
    "- $W(t)$ has independent increments.\n",
    "- $W(t)$ satisfies:\n",
    "  - $\\mathbb{E}[W(t)] = 0$,\n",
    "  - $\\text{Var}(W(t)) = t$.\n",
    "- For any $0 \\leq s < t$, the increment $W(t) - W(s) \\sim \\mathcal{N}(0, t - s)$.\n",
    "\n",
    "Thus, the scaled random walk $W^{(n)}(t)$ converges to a Brownian motion $W(t)$ as $n \\to \\infty$.\n",
    "\n",
    "--- \n",
    "\n",
    "### Log-Normal Distribution as the Limit of the Binomial Model\n",
    "\n",
    "The Central Limit Theorem can be used to show that the\n",
    "limit of a properly scaled binomial asset-pricing model leads to a stock price with a log-normal distribution.\n",
    "\n",
    "We start by considering a model for stock price on the time interval from 0 to t by choosing an integer n and constructing a binomial model for the stock price with n steps per unit time. \n",
    "\n",
    "The stock price can either go up or down. We define the up factor to be $u_n = 1 + \\frac{\\sigma}{\\sqrt(n)}$ and the down factor to be $d_n = 1 - \\frac{\\sigma}{\\sqrt(n)}$, whereas $\\sigma$ is a constant definint the volatility. \n",
    "$\n",
    "\n",
    "The stock price at time t is determined by the initial stock price S(0) and the result of the first nt coin tosses. We define nt as the sum of the number of heads $(H_{nt})$ and tails $(T_{nt})$. \n",
    "\n",
    "Furthermore, we define the random walk $M_{nt}$ to be the number of heads minus the number of tails during the first nt tosses. \n",
    "\n",
    "So we have:\n",
    "\n",
    "- $nt = H_{nt} + T_{nt}$\n",
    "- $nt = H_{nt} + T_{nt}$\n",
    "\n",
    "If we add those two equations and solve for either the number of heads or tails, we obtain:\n",
    "\n",
    "- $H_{nt} = \\frac{1}{2}(nt + M_{nt}$\n",
    "- $T_{nt} = \\frac{1}{2}(nt - M_{nt}$\n",
    "\n",
    "In our model with up factor $u_n$ and down factor $d_n$, the stock price at time t is given by:\n",
    "\n",
    "- $S_n(t) = S(0)u_n^{H_{nt}}d_n^{T_{nt}}$\n",
    "\n",
    "Which follows from the fact that the up-and-down movements are multiplied with each other to derive the cumulative time steps as cumulative product. \n",
    "\n",
    "Combining the definition of $u_n$ and $u_h$ withe the redefined formulations for number of tails and heads, we obtain:\n",
    "\n",
    "$$\n",
    "S_n(t) = S(0) \\left( 1 + \\frac{\\sigma}{\\sqrt{n}} \\right)^{\\frac{nt + M_{nt}}{2}} \\left( 1 - \\frac{\\sigma}{\\sqrt{n}} \\right)^{\\frac{nt - M_{nt}}{2}}.\n",
    "$$\n",
    "\n",
    "In the end, the scaled random walk $S^{(n)}(t)$ converges to the following distribution as $n \\to \\infty$.\n",
    "\n",
    "$$\n",
    "S(t) = S(0) \\exp\\left( \\sigma W(t) - \\frac{1}{2} \\sigma^2 t \\right).\n",
    "$$\n",
    "\n",
    "where W(t) is a normal random variable with mean zero and variance t which we call **Brownian Motion**. \n",
    "\n",
    "The distribution of S(t) in is called log-normal. More generally,\n",
    "any random variable of the form cex, where c is a constant and X is normally distributed, is said to have a log-normal distribution. \n",
    "\n",
    "In the case at hand, $X = \\sigma W(t) - \\frac{1}{2}\\sigma^2t$ is normal with mean $- \\frac{1}{2}\\sigma^2t$ and variance $\\sigma^2$.\n",
    "\n",
    "### Proof of Theorem\n",
    "\n",
    "We aim to show that the stock price in the binomial model, given by:\n",
    "$$\n",
    "S_n(t) = S(0) \\left( 1 + \\frac{\\sigma}{\\sqrt{n}} \\right)^{H_{nt}} \\left( 1 - \\frac{\\sigma}{\\sqrt{n}} \\right)^{T_{nt}},\n",
    "$$\n",
    "converges to a log-normal distribution:\n",
    "$$\n",
    "S(t) = S(0) \\exp\\left( \\sigma W(t) - \\frac{1}{2} \\sigma^2 t \\right),\n",
    "$$\n",
    "where $W(t)$ is a Brownian motion with mean 0 and variance $t$.\n",
    "\n",
    "#### Step 1: Express the logarithm of $S_n(t)$\n",
    "Taking the natural logarithm of $S_n(t)$:\n",
    "$$\n",
    "\\log S_n(t) = \\log S(0) + H_{nt} \\log\\left(1 + \\frac{\\sigma}{\\sqrt{n}}\\right) + T_{nt} \\log\\left(1 - \\frac{\\sigma}{\\sqrt{n}}\\right).\n",
    "$$\n",
    "Using the relations $H_{nt} + T_{nt} = nt$ and $M_{nt} = H_{nt} - T_{nt}$:\n",
    "$$\n",
    "\\log S_n(t) = \\log S(0) + \\frac{1}{2}(nt + M_{nt}) \\log\\left(1 + \\frac{\\sigma}{\\sqrt{n}}\\right) + \\frac{1}{2}(nt - M_{nt}) \\log\\left(1 - \\frac{\\sigma}{\\sqrt{n}}\\right).\n",
    "$$\n",
    "\n",
    "#### Step 2: Taylor series expansion of $\\log(1+x)$\n",
    "Using the Taylor series for $\\log(1+x)$:\n",
    "$$\n",
    "\\log(1+x) = x - \\frac{1}{2}x^2 + O(x^3),\n",
    "$$\n",
    "we approximate $\\log\\left(1 + \\frac{\\sigma}{\\sqrt{n}}\\right)$ and $\\log\\left(1 - \\frac{\\sigma}{\\sqrt{n}}\\right)$. For $x = \\frac{\\sigma}{\\sqrt{n}}$:\n",
    "$$\n",
    "\\log\\left(1 + \\frac{\\sigma}{\\sqrt{n}}\\right) = \\frac{\\sigma}{\\sqrt{n}} - \\frac{1}{2} \\frac{\\sigma^2}{n} + O\\left(\\frac{1}{n^{3/2}}\\right),\n",
    "$$\n",
    "and for $x = -\\frac{\\sigma}{\\sqrt{n}}$:\n",
    "$$\n",
    "\\log\\left(1 - \\frac{\\sigma}{\\sqrt{n}}\\right) = -\\frac{\\sigma}{\\sqrt{n}} - \\frac{1}{2} \\frac{\\sigma^2}{n} + O\\left(\\frac{1}{n^{3/2}}\\right).\n",
    "$$\n",
    "\n",
    "#### Step 3: Substitute the expansions into $\\log S_n(t)$\n",
    "Substituting the expansions:\n",
    "$$\n",
    "\\log S_n(t) = \\log S(0) + \\frac{1}{2}(nt + M_{nt}) \\left(\\frac{\\sigma}{\\sqrt{n}} - \\frac{1}{2} \\frac{\\sigma^2}{n} + O\\left(\\frac{1}{n^{3/2}}\\right)\\right)\n",
    "+ \\frac{1}{2}(nt - M_{nt}) \\left(-\\frac{\\sigma}{\\sqrt{n}} - \\frac{1}{2} \\frac{\\sigma^2}{n} + O\\left(\\frac{1}{n^{3/2}}\\right)\\right).\n",
    "$$\n",
    "\n",
    "Simplifying each term:\n",
    "1. For the first term:\n",
    "$$\n",
    "\\frac{1}{2}(nt + M_{nt}) \\cdot \\frac{\\sigma}{\\sqrt{n}} + \\frac{1}{2}(nt - M_{nt}) \\cdot \\left(-\\frac{\\sigma}{\\sqrt{n}}\\right) = \\frac{\\sigma}{\\sqrt{n}} M_{nt}.\n",
    "$$\n",
    "2. For the second term:\n",
    "$$\n",
    "\\frac{1}{2}(nt + M_{nt}) \\cdot \\left(-\\frac{1}{2} \\frac{\\sigma^2}{n}\\right) + \\frac{1}{2}(nt - M_{nt}) \\cdot \\left(-\\frac{1}{2} \\frac{\\sigma^2}{n}\\right) = -\\frac{1}{2} \\sigma^2 t.\n",
    "$$\n",
    "3. The $O\\left(\\frac{1}{n^{3/2}}\\right)$ terms vanish as $n \\to \\infty$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\log S_n(t) = \\log S(0) - \\frac{1}{2} \\sigma^2 t + \\frac{\\sigma}{\\sqrt{n}} M_{nt}.\n",
    "$$\n",
    "\n",
    "#### Step 4: Convergence of $M_{nt}$ to $W(t)$\n",
    "From the Central Limit Theorem, $W^{(n)}(t) = \\frac{1}{\\sqrt{n}} M_{nt}$ converges in distribution to $W(t)$, a normal random variable with mean 0 and variance $t$. As $n \\to \\infty$, the $O\\left(\\frac{1}{n^{3/2}}\\right)$ term vanishes.\n",
    "\n",
    "#### Step 5: Conclusion\n",
    "As $n \\to \\infty$, the distribution of $\\log S_n(t)$ converges to:\n",
    "$$\n",
    "\\log S(t) = \\log S(0) + \\sigma W(t) - \\frac{1}{2} \\sigma^2 t.\n",
    "$$\n",
    "Exponentiating both sides gives:\n",
    "$$\n",
    "S(t) = S(0) \\exp\\left(\\sigma W(t) - \\frac{1}{2} \\sigma^2 t\\right).\n",
    "$$\n",
    "Thus, $S(t)$ follows a **log-normal distribution**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
