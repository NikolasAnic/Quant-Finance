{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conditional Expectation\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, let $\\mathcal{G}$ be a sub-σ-algebra of $\\mathcal{F}$, and let $X$ be a random variable that is either nonnegative or integrable. The **conditional expectation** of $X$ given $\\mathcal{G}$, denoted by $\\mathbb{E}[X \\mid \\mathcal{G}]$, is any random variable that satisfies:\n",
    "\n",
    "1. **Measurability**: \n",
    "   $$\\mathbb{E}[X \\mid \\mathcal{G}] \\text{ is } \\mathcal{G}\\text{-measurable.}$$\n",
    "\n",
    "2. **Partial Averaging**: \n",
    "   For all $A \\in \\mathcal{G}$:\n",
    "   $$\n",
    "   \\int_A \\mathbb{E}[X \\mid \\mathcal{G}](\\omega) \\, dP(\\omega) = \\int_A X(\\omega) \\, dP(\\omega).\n",
    "   $$\n",
    "\n",
    "If $\\mathcal{G}$ is the σ-algebra generated by a random variable $W$ (i.e., $\\mathcal{G} = \\sigma(W)$), we typically write $\\mathbb{E}[X \\mid W]$ instead of $\\mathbb{E}[X \\mid \\sigma(W)]$.\n",
    "\n",
    "---\n",
    "\n",
    "## Conditions of Conditional Expectation\n",
    "\n",
    "### 1. **Measurability**\n",
    "\n",
    "Property (i) ensures that $\\mathbb{E}[X \\mid \\mathcal{G}]$ is a $\\mathcal{G}$-measurable random variable, meaning that the value of the estimate $\\mathbb{E}[X \\mid \\mathcal{G}]$ can be determined from the information contained in $\\mathcal{G}$. In simpler terms, $\\mathbb{E}[X \\mid \\mathcal{G}]$ is fully determined by the events in $\\mathcal{G}$.\n",
    "\n",
    "### 2. **Partial Averaging**\n",
    "Property (ii) guarantees that the conditional expectation $\\mathbb{E}[X \\mid \\mathcal{G}]$ preserves the expected value of $X$ over subsets of $\\mathcal{G}$. For example, if $\\mathcal{G}$ is generated by some random variable $W$, $\\mathbb{E}[X \\mid W]$ represents the best estimate of $X$ given the value of $W$ while maintaining consistency with $X$'s averages over $W$'s outcomes.\n",
    "\n",
    "In mathematical terms, $\\mathbb{E}[X \\mid \\mathcal{G}]$ is constant on the **atoms** of $\\mathcal{G}$ (i.e., the indivisible subsets in $\\mathcal{G}$).\n",
    "\n",
    "### Intuition Behind Conditional Expectation\n",
    "- **Measurability** ensures that the estimate $\\mathbb{E}[X \\mid \\mathcal{G}]$ can be computed using the information in $\\mathcal{G}$. \n",
    "- **Partial Averaging** ensures that $\\mathbb{E}[X \\mid \\mathcal{G}]$ is a faithful estimate of $X$ on average over events in $\\mathcal{G}$. \n",
    "- If $\\mathcal{G} = \\sigma(W)$, then $\\mathbb{E}[X \\mid W]$ uses the information provided by $W$ to estimate $X$ while maintaining consistency with $X$'s probability distribution.\n",
    "\n",
    "### Example: Conditional Expectation in a Three-Period Model\n",
    "In the context of a three-period coin toss model:\n",
    "1. $\\Omega = \\{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\\}$.\n",
    "2. $\\mathcal{F}_2$ is the σ-algebra generated by the outcomes of the first two tosses:\n",
    "   $$\n",
    "   \\mathcal{F}_2 = \\{\\emptyset, \\Omega, A_{HH}, A_{HT}, A_{TH}, A_{TT}\\},\n",
    "   $$\n",
    "   where:\n",
    "   - $A_{HH} = \\{HHH, HHT\\}$,\n",
    "   - $A_{HT} = \\{HTH, HTT\\}$,\n",
    "   - $A_{TH} = \\{THH, THT\\}$,\n",
    "   - $A_{TT} = \\{TTH, TTT\\}$.\n",
    "\n",
    "The random variable $S_3$ represents a stock price dependent on three coin tosses. The conditional expectation $\\mathbb{E}_2[S_3]$ is computed as:\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](HH) = pS_3(HHH) + qS_3(HHT),\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](HT) = pS_3(HTH) + qS_3(HTT).\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](TH) = pS_3(THH) + qS_3(THT).\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](TT) = pS_3(TTH) + qS_3(TTT).\n",
    "$$\n",
    "\n",
    "If we define the probability of Heads to be p and the probability of Tails to be q = (1-p), then we obtain the probability weighted outcomes as:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](HH)\\mathcal{P}A_{HH} = \\sum_{\\omega \\in A_{HH}}S_3(\\omega)\\mathcal{P}(\\omega)\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](HT)\\mathcal{P}A_{HT} = \\sum_{\\omega \\in A_{HT}}S_3(\\omega)\\mathcal{P}(\\omega)\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](TH)\\mathcal{P}A_{TH} = \\sum_{\\omega \\in A_{TH}}S_3(\\omega)\\mathcal{P}(\\omega)\n",
    "$$\n",
    "$$\n",
    "\\mathbb{E}_2[S_3](TT)\\mathcal{P}A_{TT} = \\sum_{\\omega \\in A_{TT}}S_3(\\omega)\\mathcal{P}(\\omega)\n",
    "$$\n",
    "\n",
    "The left-hand sides of these equations can be written as integrals of the integrand $A \\in \\mathcal{F}_2$, $\\mathbb{E}_2[S_3]$ since the conditional expectation does not depend on the third toss. The right-hand sides of these equations are sums, which are Lebesgue integrals on a finite probability space.\n",
    "\n",
    "So, for our HH case, we have that:\n",
    "\n",
    "$$\n",
    "\\int_{A_{HH}}\\mathbb{E}_2[S_3](\\omega)d\\mathcal{P}(\\omega) = \\int_{A_{HH}}S_3(\\omega)d\\mathcal{P}(\\omega)\n",
    "$$\n",
    "\n",
    "In other words, on each of the atoms the value of the conditional expectation has been chosen to be that constant that yields the same average over the atom as the random variable $S_3$ being estimated.\n",
    "\n",
    "In general, for any atom $A \\in \\mathcal{F}_2$, $\\mathbb{E}_2[S_3]$ is constant on $A$ and satisfies the partial averaging property:\n",
    "$$\n",
    "\\int_A \\mathbb{E}_2[S_3](\\omega) \\, dP(\\omega) = \\int_A S_3(\\omega) \\, dP(\\omega).\n",
    "$$\n",
    "\n",
    "This illustrates how $\\mathbb{E}[X \\mid \\mathcal{G}]$ uses the information in $\\mathcal{G}$ to estimate $X$ while maintaining its average value over events in $\\mathcal{G}$.\n",
    "\n",
    "---\n",
    "\n",
    "## Properties of Conditional Expectation\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space and let $\\mathcal{G}$ be a sub-σ-algebra of $\\mathcal{F}$. The following properties hold:\n",
    "\n",
    "### 1. **Linearity of Conditional Expectations**\n",
    "If $X$ and $Y$ are integrable random variables and $c_1$ and $c_2$ are constants, then:\n",
    "$$\n",
    "\\mathbb{E}[c_1 X + c_2 Y \\mid \\mathcal{G}] = c_1 \\mathbb{E}[X \\mid \\mathcal{G}] + c_2 \\mathbb{E}[Y \\mid \\mathcal{G}].\n",
    "$$\n",
    "This equation also holds if $X$ and $Y$ are nonnegative (rather than integrable) and $c_1$ and $c_2$ are positive, although both sides may equal $+\\infty$.\n",
    "\n",
    "### 2. **Taking Out What Is Known**\n",
    "If $X$ and $Y$ are integrable random variables, $Y$ and $XY$ are integrable, and $X$ is $\\mathcal{G}$-measurable, then:\n",
    "$$\n",
    "\\mathbb{E}[XY \\mid \\mathcal{G}] = X \\mathbb{E}[Y \\mid \\mathcal{G}].\n",
    "$$\n",
    "This equation also holds if $X$ is positive and $Y$ is nonnegative (rather than integrable), although both sides may equal $+\\infty$.\n",
    "\n",
    "### 3. **Iterated Conditioning**\n",
    "If $\\mathcal{H}$ is a sub-σ-algebra of $\\mathcal{G}$ (i.e., $\\mathcal{H}$ contains less information than $\\mathcal{G}$) and $X$ is an integrable random variable, then:\n",
    "$$\n",
    "\\mathbb{E}[\\mathbb{E}[X \\mid \\mathcal{G}] \\mid \\mathcal{H}] = \\mathbb{E}[X \\mid \\mathcal{H}].\n",
    "$$\n",
    "This equation also holds if $X$ is nonnegative (rather than integrable), although both sides may equal $+\\infty$.\n",
    "\n",
    "#### 4. **Independence**\n",
    "If $X$ is integrable and independent of $\\mathcal{G}$, then:\n",
    "$$\n",
    "\\mathbb{E}[X \\mid \\mathcal{G}] = \\mathbb{E}[X].\n",
    "$$\n",
    "This equation also holds if $X$ is nonnegative (rather than integrable), although both sides may equal $+\\infty$.\n",
    "\n",
    "#### 5. **Conditional Jensen's Inequality**\n",
    "If $\\varphi(x)$ is a convex function of a dummy variable $x$ and $X$ is integrable, then:\n",
    "$$\n",
    "\\mathbb{E}[\\varphi(X) \\mid \\mathcal{G}] \\geq \\varphi(\\mathbb{E}[X \\mid \\mathcal{G}]).\n",
    "$$\n",
    "\n",
    "### Intuitive Explanation\n",
    "1. **Linearity** ensures that the conditional expectation respects linear combinations of random variables.\n",
    "2. **Taking out what is known** highlights that $\\mathcal{G}$-measurable components can be factored out of the conditional expectation.\n",
    "3. **Iterated conditioning** guarantees consistency of nested expectations with respect to information contained in smaller σ-algebras.\n",
    "4. **Independence** reflects that the conditional expectation equals the marginal expectation when $X$ is independent of $\\mathcal{G}$.\n",
    "5. **Conditional Jensen's inequality** extends the classical Jensen's inequality to conditional expectations, showing that the convex transformation of an expectation underestimates the expectation of the convex transformation.\n",
    "\n",
    "These properties form the foundation of working with conditional expectations in both discrete and continuous probability spaces.\n",
    "\n",
    "---\n",
    "\n",
    "## Independence in Conditional Expectation\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, and let $\\mathcal{G}$ be a sub-σ-algebra of $\\mathcal{F}$. Suppose the random variables $X_1, \\ldots, X_K$ are $\\mathcal{G}$-measurable, and the random variables $Y_1, \\ldots, Y_L$ are independent of $\\mathcal{G}$. Let $f(x_1, \\ldots, x_K, y_1, \\ldots, y_L)$ be a function of the dummy variables $x_1, \\ldots, x_K$ and $y_1, \\ldots, y_L$, and define:\n",
    "$$\n",
    "g(x_1, \\ldots, x_K) = \\mathbb{E}[f(x_1, \\ldots, x_K, Y_1, \\ldots, Y_L)].\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\mathbb{E}[f(X_1, \\ldots, X_K, Y_1, \\ldots, Y_L) \\mid \\mathcal{G}] = g(X_1, \\ldots, X_K).\n",
    "$$\n",
    "\n",
    "### Intuition Behind the Lemma\n",
    "1. Since the random variables $X_1, \\ldots, X_K$ are $\\mathcal{G}$-measurable, the information in $\\mathcal{G}$ is sufficient to determine the values of $X_1, \\ldots, X_K$.\n",
    "2. The random variables $Y_1, \\ldots, Y_L$ are independent of $\\mathcal{G}$. Thus, their contribution can be \"integrated out\" without any dependence on the information in $\\mathcal{G}$.\n",
    "3. The function $g(x_1, \\ldots, x_K)$, defined as the expected value of $f(x_1, \\ldots, x_K, Y_1, \\ldots, Y_L)$ over the distribution of $Y_1, \\ldots, Y_L$, captures this integration step.\n",
    "4. Finally, the result depends on the values of $X_1, \\ldots, X_K$, which are then replaced by their corresponding random variables to yield the random variable $\\mathbb{E}[f(X_1, \\ldots, X_K, Y_1, \\ldots, Y_L) \\mid \\mathcal{G}]$.\n",
    "\n",
    "### Key Takeaway\n",
    "This lemma highlights how independence between random variables and a σ-algebra simplifies conditional expectations. The contribution of independent random variables is integrated out, leaving a dependence only on the $\\mathcal{G}$-measurable random variables.\n",
    "\n",
    "--- \n",
    "## Martingales, Submartingales, and Supermartingales\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, let $T$ be a fixed positive number, and let $\\mathcal{F}(t)$, $0 \\leq t \\leq T$, be a filtration of sub-σ-algebras of $\\mathcal{F}$. Consider an adapted stochastic process $M(t)$, $0 \\leq t \\leq T$. The definitions of martingales, submartingales, and supermartingales capture how the process $M(t)$ evolves over time with respect to the information captured by the filtration $\\mathcal{F}(t)$.\n",
    "\n",
    "#### (i) Martingale\n",
    "The process $M(t)$ is a **martingale** if for all $0 \\leq s \\leq t \\leq T$:\n",
    "$$\n",
    "\\mathbb{E}[M(t) \\mid \\mathcal{F}(s)] = M(s).\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- A martingale represents a \"fair game\" where the conditional expected future value of the process, given all the information up to time $s$ (encoded in $\\mathcal{F}(s)$), is equal to the value of the process at time $s$.\n",
    "- There is **no tendency for the process to rise or fall** over time.\n",
    "- Martingales are central in financial modeling, particularly for modeling fair asset prices in the absence of arbitrage.\n",
    "\n",
    "**Intuition:**\n",
    "The process reflects a situation where, knowing the present, the best estimate of the future value is the current value. Examples include stock prices under certain conditions in efficient markets.\n",
    "\n",
    "#### (ii) Submartingale\n",
    "The process $M(t)$ is a **submartingale** if for all $0 \\leq s \\leq t \\leq T$:\n",
    "$$\n",
    "\\mathbb{E}[M(t) \\mid \\mathcal{F}(s)] \\geq M(s).\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- A submartingale reflects a process with **no tendency to decrease** over time; it may have a tendency to increase.\n",
    "- The conditional expected value of the future process, given information up to time $s$, is at least as large as its current value.\n",
    "\n",
    "**Intuition:**\n",
    "The process has a built-in upward bias, such as a stock price expected to grow due to positive drift or a betting scenario where the odds are in your favor.\n",
    "\n",
    "#### (iii) Supermartingale\n",
    "The process $M(t)$ is a **supermartingale** if for all $0 \\leq s \\leq t \\leq T$:\n",
    "$$\n",
    "\\mathbb{E}[M(t) \\mid \\mathcal{F}(s)] \\leq M(s).\n",
    "$$\n",
    "\n",
    "**Explanation:**\n",
    "- A supermartingale reflects a process with **no tendency to increase** over time; it may have a tendency to decrease.\n",
    "- The conditional expected value of the future process, given information up to time $s$, is at most as large as its current value.\n",
    "\n",
    "**Intuition:**\n",
    "The process has a built-in downward bias, such as a depreciating asset or a betting scenario where the odds are against you.\n",
    "\n",
    "### Key Points to Note\n",
    "1. **Adaptation to Filtration:**\n",
    "   - The process $M(t)$ is **adapted** to the filtration $\\mathcal{F}(t)$, meaning the value of $M(t)$ at any time $t$ is determined by the information available up to time $t$.\n",
    "\n",
    "2. **Filtration $\\mathcal{F}(t)$:**\n",
    "   - This is a family of σ-algebras representing the accumulation of information over time. At time $t$, $\\mathcal{F}(t)$ captures all information available up to and including $t$.\n",
    "\n",
    "3. **Comparison:**\n",
    "   - Martingale: No bias in the evolution of $M(t)$.\n",
    "   - Submartingale: Potential upward bias.\n",
    "   - Supermartingale: Potential downward bias.\n",
    "\n",
    "## Markov Process\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space, let $T$ be a fixed positive number, and let $\\mathcal{F}(t)$, $0 \\leq t \\leq T$, be a filtration of sub-σ-algebras of $\\mathcal{F}$. Consider an **adapted stochastic process** $X(t)$, $0 \\leq t \\leq T$. We say that $X(t)$ is a **Markov process** if for all $0 \\leq s \\leq t \\leq T$, and for every nonnegative, Borel-measurable function $f$, there exists another Borel-measurable function $g$ such that:\n",
    "$$\n",
    "\\mathbb{E}[f(X(t)) \\mid \\mathcal{F}(s)] = g(X(s)). \\tag{2.3.29}\n",
    "$$\n",
    "\n",
    "### Explanation\n",
    "The **Markov property** characterizes a process where the future evolution of the process depends on its current state but **not on its past history**, given the present. \n",
    "\n",
    "1. **Conditioning on the Past:**\n",
    "   - $\\mathcal{F}(s)$ contains all the information available up to time $s$.\n",
    "   - The Markov property implies that the conditional expectation of any function of $X(t)$ (future state) depends only on $X(s)$ (current state) and not on how the process arrived at $X(s)$.\n",
    "\n",
    "2. **Functions $f$ and $g$:**\n",
    "   - $f$ is any nonnegative, Borel-measurable function applied to the state at time $t$.\n",
    "   - $g$ is another Borel-measurable function that captures the dependence of the conditional expectation on the current state $X(s)$.\n",
    "\n",
    "3. **Adaptation:**\n",
    "   - The process $X(t)$ is adapted to the filtration $\\mathcal{F}(t)$, meaning that $X(t)$ depends only on the information available up to time $t$.\n",
    "\n",
    "### Remark: Time Dependence\n",
    "In the Markov property, the functions $f$ and $g$ can explicitly depend on time:\n",
    "\n",
    "- By writing $f(t, x)$ instead of $f(x)$, we emphasize the dependence of $f$ on both the time $t$ and the state $x$.\n",
    "- Similarly, $g$ can depend on $s$, written as $f(s, x)$ to reflect the time and state dependence at the earlier time.\n",
    "\n",
    "Using this notation, we can rewrite Equation (2.3.29) as:\n",
    "$$\n",
    "\\mathbb{E}[f(t, X(t)) \\mid \\mathcal{F}(s)] = f(s, X(s)), \\quad 0 \\leq s \\leq t \\leq T. \\tag{2.3.30}\n",
    "$$\n",
    "\n",
    "This version of the Markov property highlights that:\n",
    "- The conditional expectation at time $t$ (given the past up to time $s$) depends only on the value of $X(s)$ and is determined by a function $f$ evaluated at $(s, X(s))$.\n",
    "\n",
    "### Connection to Partial Differential Equations\n",
    "The Markov property leads to a partial differential equation (PDE) when $f(t, x)$ is treated as a function of two variables (time $t$ and state $x$). \n",
    "\n",
    "#### Key Insight:\n",
    "If we know $f(t, x)$ at time $t$, we can use the Markov property to determine $f(s, x)$ at an earlier time $s$. This relationship is often governed by a PDE.\n",
    "\n",
    "#### Example:\n",
    "- In financial mathematics, the **Black-Scholes-Merton PDE** is a specific example of how the Markov property applies to pricing derivative securities. The PDE relates the price of a derivative (as a function of time and underlying asset price) to its boundary conditions and the dynamics of the underlying asset.\n",
    "\n",
    "\n",
    "### Summary of Markov Property\n",
    "| **Key Feature**               | **Explanation**                                                                 |\n",
    "|--------------------------------|---------------------------------------------------------------------------------|\n",
    "| **Memoryless Property**        | Future depends only on the present state, not the past history.                 |\n",
    "| **Conditional Expectation**    | $\\mathbb{E}[f(X(t)) \\mid \\mathcal{F}(s)] = g(X(s))$.                            |\n",
    "| **Time Dependence**            | The functions $f$ and $g$ may explicitly depend on time: $f(t, x)$ and $f(s, x)$. |\n",
    "| **Connection to PDEs**         | The Markov property leads to PDEs that describe the evolution of $f(s, x)$.    |\n",
    "\n",
    "The Markov process provides a foundation for modeling systems where future behavior depends only on the current state, making it a crucial concept in probability theory, statistics, and mathematical finance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
